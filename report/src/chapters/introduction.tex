\section{Introduction}
\label{sec:introduction}

% Author: Silvana

Automatically classifying and analyzing big amount of digital content has become a common task in the last decades. In case of digital media content, data is already present in quantified form, or eventually has to be quantified before it can be processed. In either case, classification can be a challenge, as the amount of data in many applications is vast: not only that many samples are taken, but those samples often consist of many features. In machine learning, this problem is known as the so-called "curse of dimensionality": The complexity of the analysis grows exponentially with increasing dimensionality of the samples. Furthermore, many algorithms tend to over-fit when too much information is given, which is another drawback of big feature sets. \cite{Verikas:02}

Those problems are the motivation for finding methods to gain expressive representations of datasets - for example by removing redundant and irrelevant features. This allows not only a more accurate classification, but also speeds up computation time. There are different strategies on how to reduce the dimensionality of the feature set, and this paper will focus exclusively on one of them: feature selection. The core idea of feature selection is to select a relevant subset of the already existing feature set (instead of introducing new features, or mapping them to another space). The definition of "relevant features" is of course heavily dependent on the application, but generally speaking, discriminant features are relevant, because they facilitate clear classification. Furthermore, features should not be redundant.

This paper aims to present some of the most common techniques and state-of-the art methods in feature classification. The presented methods do not only apply to media classification (e.g. document classification, text analysis in social media, video analysis), but also to a broad range of problems emerging in different scientific fields (e.g. analysis of genomes in bio informatics).

In chapter 2, a short categorization of features according to their assumed structure is given. Then, methods which are suited for the different feature types are introduced. First, flat features and filter, wrapper and embedded methods are described. Then, applicable methods for structured features and streaming features are presented. Finally, a discussion regarding the presented methods is made in chapter 3.



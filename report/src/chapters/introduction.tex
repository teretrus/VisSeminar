\section{Introduction}
\label{sec:introduction}

% Author: Silvana

Thanks to the technological development of the recent years, nowadays it is possible to collect and store a vast amount of data. High-dimensional data is omnipresent in a broad range of real-life applications, such as text analysis in social media or the analysis of genomes in bio informatics. But these possibilities also led to new challenges when evaluating those datasets. Not only the sample size, also the dimensionality of data has increased, which is the cause for the so called "curse of dimensionality": The complexity of the analysis (regarding memory consumption and computation time) grows exponentially with increasing dimensionality of the samples. Furthermore, many algorithms tend to overfit when too many sample points are given, which is another drawback of big feature sets. \cite{2001:Feature selection with NN, p1333}

To conquer this problem, various techniques to reduce dimensionality were developed in the field of machine learning. Two broad categories can be distinguished: feature extraction and feature selection. 
In feature extraction, features are combined to create "new" features in a lower dimensional space. Feature selection on the other hand tries to select a few, important features out of the given set.
The features produced or chosen by either one of those methods are then used for classification and analysis of the sampled data. 

The goal of this paper is to set a focus on feature selection and present some of the most common techniques and state-of-the art.

The main strategy of feature selection is to chose only a small subset of the available features for analyzing the data, which leads to higher accuracy, lower computational cost and better interpretability of the results.\cite{} Such a subset would optimally consist only of relevant features, whereas redundant or useless features should be removed. What properties define a relevant feature depends on the application, but generally speaking, the optimal feature should be discriminant when performing classification (so when looking at a certain feature, a clear classification of the current data point should be possible)
and should not be redundant (two different features should not provide the same kind of information). Possible criteria for evaluating the relevance of features will be covered in more detail in section FOO.


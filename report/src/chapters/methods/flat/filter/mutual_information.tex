\paragraph{Mutual information based}
\label{par:methods.flat.filter.mutual_information}

% Author: Flo - Silvana haut auch was rein
  
Mutual information  = how much one variable tells us about another- No mutual information meann they are independent, high mutual information means they are dependent on each other - in other words, quantifies how dependent two variables are. text-example: how often does t appear with c, t appears without c, and c appears without t, and total sample number --> calculate mutual info we have

Different method based on this idea like min redundancy max relevance, information gain

mRmr = combine measures for redundancy and relevance, select features with high relevance and minimize redundancy.

infogain = feature given and a corresponding class distribution - measure amount of information regarding class prediction when this feature is given - select features which contribute high information gain
text processing paper: measure the amount of information in bits obtained for classification by knowing the absence or presence of a term in a document- terme = features
\paragraph{Relief}
\label{par:methods.flat.filter.relief}

% Author: Silvana
The original Relief-algorithm was presented by Kira and Rendell \cite{Kira:92}, and can be used for binary classification problems. The algorithm is efficient (it runs in polynomial time), robust and tolerant to noisy data, but the quality of the results is dependent on the number of iterations performed.

The method operates on the whole trainingset of samples and the features each sample consists of. A vector of weights is used, where each weight represents a feature. In each iteration, one of the samples (further referred to as x) is chosen at random. Then the near-hit and near-miss instance of the remaining set in relation to x are selected: The first is the most similar instance from the same class as x, and the latter is the most similar instance of the different class. The distance between the features of x and the near-hit and x and the near-miss is calculated to determine if a feature is relevant or not.  

Intuitively, if a feature is relevant, the near-hits should be closer than the near-misses on average, while for irrelevant features, near-hits and near-misses are very similar to each other and are not very discriminative. For each feature $f_i$, the corresponding weight $w_i$ in the weight-vector is updated dependent on the euclidean distances by applying equation \ref{relief}:

\begin{equation}
\label{relief}
	w_i = w_i - (x_i - nearhit_i)^2  + (x_i - nearmiss_i)^2
\end{equation}

Relevant features thus score values larger than zero, while irrelevant features become zero or negative. After a desired number of iterations, the features whose weights have a value above a certain threshold are chosen for the classification or other processing routines.

The original algorithm was further improved. ReliefF \cite{Kononenko:97} is an extension so that the algorithm is able to handle incomplete data and multiple-class problems. 
RRELIEFF (Regressional ReliefF) \cite{Robnik-Sikonja:97} adapts the algorithm to handle also linear regression problems.

The family of Relief-algorithms and eventual adaptations has successfully been used on feature selection problems in the recent years. Moore used an adaptation called Tuned ReliefF (TuRF) for genetic analysis \cite{Moore:07}, where the worst features are removed systematically in each iteration. By removing a fixed number of features in each iteration, the accuracy of weight-estimation is increasing compared to the results of the original ReliefF algorithm. Eppstein and Haake \cite{Eppstein:08} criticize that for truly genome-wide association analysis, where a huge number of features is used (up to hundreds of thousands), all the proposed methods don't scale well and the estimated weights become basically random values. Very large scale ReliefF (VLSReliefF) tries to overcome this limitation by simply applying Relief on subsets of the features, and then combining the results to gain the weights for all features. This process can be parallelized on the GPU to speed it up \cite{Lee:15}
In the context of media classification, the ReliefF algorithm was used for automated classification of websites \cite{Jin:07}.

